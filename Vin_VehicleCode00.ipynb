{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File tm_vin.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9143fcd40b82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtm_vin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tm_vin.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvin_vehicleCode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vin.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'feature_list_version.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tm_feature.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File tm_vin.csv does not exist"
     ]
    }
   ],
   "source": [
    "tm_vin=pd.read_csv('tm_vin.csv')\n",
    "vin_vehicleCode=pd.read_csv('vin.csv')\n",
    "version=pd.read_csv('feature_list_version.csv')\n",
    "feature=pd.read_csv('tm_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is our data from shanghai insurence company\n",
    "#few of them may be wrong,but just a few.\n",
    "print len(vin_vehicleCode)\n",
    "vin_vehicleCode.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is the EPC from the company \n",
    "print len(tm_vin)\n",
    "tm_vin.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is what we want to show to the insurane company\n",
    "vehicle_code=pd.read_csv('vehicle_code.csv')\n",
    "vehicle_code.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print vehicle_code['VEHICLE_CODE'].is_unique\n",
    "print 'we have different kinds of vehicle code from the EPC:',len(vehicle_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the number of vehicle code from the original data of insurance company\n",
    "vin4=vin_vehicleCode.groupby('VEHICLE_CODE',as_index=False)['VEHICLE_MODEL'].count()\n",
    "\n",
    "print 'how many vehicle codes  from the insurance company:',len(vin4)\n",
    "#the number of vehicle code which have no more than 10 cars for each vehicle code\n",
    "# print 'how many vehicle code which have no more than 10 cars from the insurance company:',len(vin4[vin4['VEHICLE_MODEL']<=1])\n",
    "# print vin4.sort('VEHICLE_MODEL')\n",
    "\n",
    "# easy code to show \n",
    "# print vin_vehicleCode['VEHICLE_CODE'].value_counts()\n",
    "print 'how many vehicle code which have no more than 10 cars from the insurance company:',np.sum(vin_vehicleCode['VEHICLE_CODE'].value_counts()<2)\n",
    "print 'this data will cause problme when trainning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vin_vehicleCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check wheather all the vehicle code from the insurance company is in the EPC\n",
    "# all(xx in vin4['VEHICLE_CODE'] if xx in vehicle_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let us have a look at the feature list version csv and the feature list\n",
    "feature_list_version= pd.read_csv('FEATURE_LIST_VERSION.csv')\n",
    "print 'we have different features:',len(feature_list_version)\n",
    "feature_list_version.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print feature_list_version['FEATURE_LIST'][0]\n",
    "print 'the length of feature list',len(feature_list_version['FEATURE_LIST'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now we want to interpret the feature list to mapping the vehicle code\n",
    "\n",
    "#1-4:   2     year\n",
    "#5-8:   3     export country\n",
    "#4bits: 5     colour \n",
    "#4bits: 6     inside colour\n",
    "#4bits: 7     vin\n",
    "#3-4bits:A00  car style\n",
    "\n",
    "tm_feature = pd.read_csv('tm_feature.csv')\n",
    "tm_feature[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we want to have a look at the number of levels for each family code(categorical feature)\n",
    "b = tm_feature.groupby('FAMILY_CODE',as_index=False).count()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print type(b)\n",
    "print 'we have different features from the feature list corresponding to each vin code: ',len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## filter the feature with family code\n",
    "\n",
    "#b['FAMILY_CODE']\n",
    "# i=0\n",
    "# # b['FAMILY_CODE'][i]\n",
    "# i=i+1\n",
    "# print b['FAMILY_CODE'][i]\n",
    "# tm_feature[tm_feature['FAMILY_CODE']==b['FAMILY_CODE'][i]]\n",
    "#tm_feature[tm_feature['FAMILY_CODE']=='A10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##check the information from the insurance company and the EPC database\n",
    "#WE want to use this information to construct a tranning data: featuers(from feature list vesrsion id to feature list,then to \n",
    "#feature) and Vehicle_Code(as lable)\n",
    "vin_conbined=pd.merge(vin_vehicleCode,tm_vin,left_on='RACK_NO',right_on='VIN_CODE',how='inner')\n",
    "print 'the num of obsevations from the insurance compay acturall in our EPC database:',len(vin_conbined), '\\n'\n",
    "print 'we have the obsevations from the insurance company:',len(vin_vehicleCode)\n",
    "#some of the rack_code from the insurance company is not exist in our EPC database\n",
    "vin_conbined.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we want to try to find the vehicle code with the vin code from the insurance company\n",
    "#if it is in the EPC database:\n",
    "#but no vehicle code is directly matched, we want to predict the vehicle code by classification method with the corresponding \n",
    "#featues.so we can search in the EPC database to find the feature list by vin code and interpret it as the correspongding features\n",
    "#which can help us to predict the vehicle code we had.\n",
    "\n",
    "#first,we need to construct a trainning data from the data of insurance company,and trainning a 'good' algorithm to predict:\n",
    "#interpret the feature list into lots of features for each car from insurance company, then make it as trainning data to construct some alogrithm to predict \n",
    "\n",
    "\n",
    "#but if the vin code from the insurance company is not in the EPC database(becaues of the delay of database):\n",
    "#we cannt get the featuers from our EPC database,then we have to predict the vehicle code just by vin code. one way is just treat the vin code as one singel feature to predict,\n",
    "#another way is to interpret the vin code into meanningful features to predict.(the question is how?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vin2=vin1[['VEHICLE_CODE','FEATURE_LIST_VERSION_ID']]\n",
    "# print len(vin2)\n",
    "# vin2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vin2['num']=0\n",
    "# vin2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vin3=vin1.groupby(['FEATURE_LIST_VERSION_ID','VEHICLE_CODE'],as_index=False)['VEHICLE_MODEL'].count()\n",
    "# print len(vin3)\n",
    "# vin3.head(10)\n",
    "#we find the vehicle code is not unique for each feature list version \n",
    "\n",
    "#vin3['num']=vin3['num'].astype(int)\n",
    "#vin3[['FEATURE_LIST_VERSION_ID','VEHICLE_CODE','NUM']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now we want to construct a trainning data with 79389 obsevations (actually in the EPC database)and 349 features and the vehicle code as label\n",
    "vin_conbined.head()\n",
    "raw_data =vin_conbined[['RACK_NO','VEHICLE_CODE','VEHICLE_MODEL','PLATFORM_CODE','FEATURE_LIST_VERSION_ID']]\n",
    "print len(raw_data)\n",
    "raw_data =pd.merge(raw_data,feature_list_version,on='FEATURE_LIST_VERSION_ID')\n",
    "print  'construct a trainning data with 79389 obsevations (actually in the EPC database',len(raw_data)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set the column of the train data\n",
    "dict_column = tm_feature.groupby('FAMILY_CODE',as_index=False).count()['FAMILY_CODE']\n",
    "dict_column= list(dict_column)\n",
    "print type(dict_column)\n",
    "print len(dict_column)\n",
    "#print dict_column\n",
    "\n",
    "\n",
    "#set the empty train data frame\n",
    "train_data = pd.DataFrame(columns=dict_column)\n",
    "train_data_CN = pd.DataFrame(columns=dict_column)\n",
    "print train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "#then we want to interpret the feature list into 349 features\n",
    "print  'we show one sample to interpre the feature list to 349 features:'\n",
    "sample = raw_data['FEATURE_LIST'][0]\n",
    "print sample\n",
    "type(sample)\n",
    "len(sample)\n",
    "sample[4:8]\n",
    "length =len(sample)\n",
    "\n",
    "temp = tm_feature[tm_feature['FEATURE_CODE']==sample[0:0+4]]['FEATURE_CN_NAME']\n",
    "print temp[temp.index[0]]\n",
    "# print type(tm_feature[tm_feature['FEATURE_CODE']==sample[0:0+4]]['FEATURE_CN_NAME'])\n",
    "# print tm_feature[tm_feature['FEATURE_CODE']==sample[0:0+4]]['FEATURE_CN_NAME'] # it output a series with index\n",
    "#     index =tm_feature[tm_feature['FEATURE_CODE']==sample[0:0+4]].index\n",
    "\n",
    "for x in range(0,len(sample),4):\n",
    "         print x, sample[x:x+4],\n",
    "         print tm_feature[tm_feature['FEATURE_CODE']==sample[x:x+4]]['FEATURE_CN_NAME']      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#samle is a feature list\n",
    "temp_data = pd.DataFrame(np.nan, index=[0],columns=dict_column)\n",
    "temp_data_CN = pd.DataFrame(np.nan, index=[0],columns=dict_column)\n",
    "\n",
    "feature_list = raw_data['FEATURE_LIST'][0]\n",
    "# print type(feature_list)\n",
    "# print feature_list\n",
    "# print temp_data\n",
    "# print type(dict_column)\n",
    "# print type(feature_list[0])\n",
    "count =0\n",
    "for xx in range(0,len(feature_list),4):\n",
    "    \n",
    "    if (feature_list[xx] in ['2','3','5','6','7']) and (count <=6):\n",
    "        count = count+1\n",
    "        feature =feature_list[xx]\n",
    "        feature_code =feature_list[xx:xx+4]\n",
    "        #print xx,count,feature_code\n",
    "        #print feature_code\n",
    "        \n",
    "        if tm_feature[tm_feature['FEATURE_CODE']==feature_code].values.shape[0]==0:\n",
    "            #some feature_code like 76E3 in 279th observation ,but not exsit in the tm_feature['FEATURE_CODE], we assume it is just the correce \n",
    "            #feature_code,but fail to find the chinese feature name\n",
    "            if tm_feature[tm_feature['FEATURE_CODE']==feature_code[0:3]].values.shape[0]==0:\n",
    "                temp_data[feature] = np.nan\n",
    "                temp_data_CN[feature]= np.nan\n",
    "                          \n",
    "            #some feature_code lke G10N in 0th observation, but not exsit in the tm_feature['FEATURE_CODE],\n",
    "            #we assume it is the same as G10    \n",
    "            else:\n",
    "                temp_data[feature] = feature_code[0:3]\n",
    "                temp_data_CN[feature]= feature_code[0:3]\n",
    "         \n",
    "        #print feature_code\n",
    "        else:\n",
    "            temp_data[feature] = feature_code\n",
    "            #print feature_code, tm_feature[tm_feature['FEATURE_CODE']==feature_code]\n",
    "            temp_data_CN[feature]= tm_feature[tm_feature['FEATURE_CODE']==feature_code].values[(0,4)]\n",
    "            #.values which change a data frame into np array\n",
    "\n",
    "    elif any(feature_list[xx:xx+3] == x for x in dict_column):\n",
    "        #print any(feature_list[xx:xx+3] == x for x in dict_column)\n",
    "        feature = feature_list[xx:xx+3]\n",
    "        feature_code = feature_list[xx:xx+4]\n",
    "        \n",
    "        if tm_feature[tm_feature['FEATURE_CODE']==feature_code].values.shape[0]==0:\n",
    "            #some feature_code like 76E3 in 279th observation ,but not exsit in the tm_feature['FEATURE_CODE], \n",
    "            #2017/7/18 now I find the error is come from the transformation from database to csv, \n",
    "            #76E3 will be transformed into scientific notation 7.60E+04\n",
    "            #76E9 will be transformed into scientific notation 7.60E+10\n",
    "            \n",
    "            # 76E6    7.60E+07\n",
    "            #74E3 will be transformed into scientific notation 7.40E+04\n",
    "            # 74E4    7.40E+05\n",
    "\n",
    "\n",
    "            #so when we transform the data fromthe database to csv file, we need to make sure the data tape as string \n",
    "            \n",
    "            \n",
    "            \n",
    "            #we assume it is just the correce \n",
    "            #feature_code,but fail to find the chinese feature name\n",
    "            if tm_feature[tm_feature['FEATURE_CODE']==feature_code[0:3]].values.shape[0]==0:\n",
    "                print 'feature code is not exist: '+ featrue_code\n",
    "                temp_data[feature] = np.nan\n",
    "                temp_data_CN[feature]= np.nan\n",
    "                \n",
    "                \n",
    "            #some feature_code lke G10N in 0th observation, but not exsit in the tm_feature['FEATURE_CODE],\n",
    "            #we assume it is the same as G10    \n",
    "            else:\n",
    "                temp_data[feature] = feature_code[0:3]\n",
    "                temp_data_CN[feature]= feature_code[0:3]\n",
    "         \n",
    "        #print feature_code\n",
    "        else:\n",
    "            temp_data[feature] = feature_code\n",
    "            #print feature_code, tm_feature[tm_feature['FEATURE_CODE']==feature_code]\n",
    "            temp_data_CN[feature]= tm_feature[tm_feature['FEATURE_CODE']==feature_code].values[(0,4)]\n",
    "            #.values which change a data frame into np array\n",
    "            \n",
    "        #print type(tm_feature[tm_feature['FEATURE_CODE']==feature_code]),feature_code\n",
    "        #print tm_feature[tm_feature['FEATURE_CODE']==feature_code].values.shape[0]\n",
    "    \n",
    "                  \n",
    "    \n",
    "#     print temp\n",
    "#     print type(temp)\n",
    "    #print str(temp[temp.index[0]])# a wierd situation here\n",
    "    #print temp[0:1]\n",
    "    #temp_data_CN[feature]= tempCN\n",
    "    \n",
    "#the feature code version of feature for one car\n",
    "print temp_data\n",
    "#the chinese verion of feature for one car\n",
    "print temp_data_CN\n",
    "\n",
    "\n",
    "# we can use other 'FEATURE_CN_CHINA' OR 'NUM' HERE         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To improve the efficiency of interpret the vin code into the features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list_raw_data = raw_data['FEATURE_LIST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #now we want to do the same process to every car from the insurance company to build a train data, extract every car' feature list into a \n",
    "# #dataframe \n",
    "# num_vehicle = len(raw_data)\n",
    "# #set the timer\n",
    "# import timeit\n",
    "# start_time= timeit.default_timer()\n",
    "\n",
    "# #test for a few observations\n",
    "# # num_vehicle_test=2000\n",
    "\n",
    "# #training for the whole data\n",
    "# num_vehicle_test = num_vehicle\n",
    "\n",
    "\n",
    "\n",
    "# for x in range(num_vehicle_test):\n",
    "#     #set a empty temp_data for each vehicle\n",
    "#     temp_data = pd.DataFrame(np.nan, index=[0],columns=dict_column)\n",
    "#     temp_data_CN = pd.DataFrame(np.nan, index=[0],columns=dict_column)\n",
    "#     #choose the feature list for each vehicle\n",
    "    \n",
    "# #     feature_list = raw_data['FEATURE_LIST'][x]\n",
    "#     feature_list = feature_list_raw_data[x]  # which make you do not need to read the whole table each time\n",
    "    \n",
    "#     #extract the features from the feature_list for each vehicle into the dataframe named temp_data or temp_data_CN\n",
    "#     count =0\n",
    "    \n",
    "    \n",
    "#     for xx in range(0,len(feature_list),4):\n",
    "\n",
    "#         if (feature_list[xx] in ['2','3','5','6','7']) and (count <=6):\n",
    "#             count = count+1\n",
    "#             feature =feature_list[xx]\n",
    "#             feature_code =feature_list[xx:xx+4]\n",
    "#             #print xx,count,feature_code\n",
    "#             #print feature_code\n",
    "\n",
    "#             if tm_feature[tm_feature['FEATURE_CODE']==feature_code].values.shape[0]==0:\n",
    "#                 #some feature_code like 76E3 in 279th observation ,but not exsit in the tm_feature['FEATURE_CODE], we assume it is just the correce \n",
    "#                 #feature_code,but fail to find the chinese feature name\n",
    "#                 if tm_feature[tm_feature['FEATURE_CODE']==feature_code[0:3]].values.shape[0]==0:\n",
    "#                     temp_data[feature] = np.nan\n",
    "#                     temp_data_CN[feature]= np.nan\n",
    "\n",
    "#                 #some feature_code lke G10N in 0th observation, but not exsit in the tm_feature['FEATURE_CODE],\n",
    "#                 #we assume it is the same as G10    \n",
    "#                 else:\n",
    "#                     temp_data[feature] = feature_code[0:3]\n",
    "#                     temp_data_CN[feature]= feature_code[0:3]\n",
    "\n",
    "#             #print feature_code\n",
    "#             else:\n",
    "#                 temp_data[feature] = feature_code\n",
    "#                 #print feature_code, tm_feature[tm_feature['FEATURE_CODE']==feature_code]\n",
    "#                 temp_data_CN[feature]= tm_feature[tm_feature['FEATURE_CODE']==feature_code].values[(0,4)]\n",
    "#                 #.values which change a data frame into np array\n",
    "\n",
    "#         elif any(feature_list[xx:xx+3] == x for x in dict_column):\n",
    "#             #print any(feature_list[xx:xx+3] == x for x in dict_column)\n",
    "#             feature = feature_list[xx:xx+3]\n",
    "#             feature_code = feature_list[xx:xx+4]\n",
    "\n",
    "#             if tm_feature[tm_feature['FEATURE_CODE']==feature_code].values.shape[0]==0:\n",
    "#                 #some feature_code like 76E3 in 279th observation ,but not exsit in the tm_feature['FEATURE_CODE], \n",
    "#                 #2017/7/18 now I find the error is come from the transformation from database to csv\n",
    "                \n",
    "#                 #we assume it is just the correce feature_code,but fail to find the chinese feature name\n",
    "#                 if tm_feature[tm_feature['FEATURE_CODE']==feature_code[0:3]].values.shape[0]==0:\n",
    "#                     temp_data[feature] = np.nan\n",
    "#                     temp_data_CN[feature]= np.nan\n",
    "\n",
    "\n",
    "#                 #some feature_code lke G10N in 0th observation, but not exsit in the tm_feature['FEATURE_CODE],\n",
    "#                 #we assume it is the same as G10    \n",
    "#                 else:\n",
    "#                     temp_data[feature] = feature_code[0:3]\n",
    "#                     temp_data_CN[feature]= feature_code[0:3]\n",
    "\n",
    "#             #print feature_code\n",
    "#             else:\n",
    "#                 temp_data[feature] = feature_code\n",
    "#                 #print feature_code, tm_feature[tm_feature['FEATURE_CODE']==feature_code]\n",
    "#                 temp_data_CN[feature]= tm_feature[tm_feature['FEATURE_CODE']==feature_code].values[(0,4)]\n",
    "#                 #.values which change a data frame into np array\n",
    "   \n",
    "#     #test temp_data every 100 obsevations    \n",
    "#     if x%100==0:\n",
    "#         print x,temp_data_CN    \n",
    "#     #append the temp data into the big train data\n",
    "#     train_data = train_data.append(temp_data)\n",
    "#     train_data_CN=train_data_CN.append(temp_data_CN)\n",
    "    \n",
    "# end_time= timeit.default_timer()\n",
    "# run_time =end_time-start_time\n",
    "# print run_time\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run_time = 144652.977632 =1.6742242781509526 days\n",
    "# #because the data is huge,we want to save it into csv file to resues later\n",
    "# train_data.to_csv('train_data.csv',encoding='utf-8',index= False)\n",
    "# train_data_CN.to_csv('train_data_CN.csv',encoding='utf-8',index= False)\n",
    "\n",
    "\n",
    "# we can read the train data from the file we stored befor then you dont need to run the code above to generate the train data\n",
    "train_data= pd.read_csv('train_data.csv') \n",
    "# when we store a table ,we will store the index into the 1st column,so when we read,we need to tell the computer which column is index column \n",
    "# train_data= pd.read_csv('train_data.csv',index_col=0)\n",
    "\n",
    "train_data_CN =pd.read_csv('train_data_CN.csv')\n",
    "\n",
    "# saving datafames to an excel workbook\n",
    "# writer = pd.ExcelWriter('train_data.xlsx')\n",
    "# train_data.to_excel(writer,'sheet1')\n",
    "# writer.save()\n",
    "# train_data.columns\n",
    "# train_data.drop(train_data.columns[0],inplace= True,axis =1)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_CN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(raw_data)\n",
    "#print num_vehicle_test\n",
    "print train_data.shape\n",
    "print train_data_CN.shape\n",
    "#train_data\n",
    "#train_data[100:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check the table we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vin_table= pd.read_csv('vin.csv')\n",
    "vin_table[vin_table['RACK_NO']=='LSJW26H38AS072686']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm_vin[tm_vin['VIN_CODE']=='LSJW26H38AS072686']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list_version[feature_list_version['FEATURE_LIST_VERSION_ID']==1000036275]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) EDA about the raw data from the insurance compay adn out EPC database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We want to inspect the distribution for each plat form\n",
    "platform_groups = raw_data.groupby('PLATFORM_CODE_x',as_index=True).VEHICLE_CODE.count()\n",
    "print 'we have {0} kinds of platforms from the insurance company which is in our EPC '.format(len(platform_groups))\n",
    "platform_groups.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) EDA about the train data with features transformed from the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print train_data.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.head()\n",
    "# figure = train_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1. Preprocessing (80% of work)\n",
    "### Agenda:(from April Chen)\n",
    "1.1. Basic Data Cleaning\n",
    "    1. Dealing with data types\n",
    "    2. Handling missing data\n",
    "1.2. More Data Exploration\n",
    "    1. Outlier detection\n",
    "    2. Plotting distributions\n",
    "1.3. Feature Engineering\n",
    "    1. Interactions between features\n",
    "    2. Dimensionality reduction using PCA\n",
    "1.4. Feature Selection and Model Building\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Basic Data Cleaning\n",
    "### A. Dealing with data types\n",
    "\n",
    "- There are three main data types:\n",
    "    - Numeric, e.g. income, age\n",
    "    - Categorical, e.g. gender, nationality \n",
    "    - Ordinal, e.g. low/medium/high\n",
    "    \n",
    "    \n",
    "- Models can only handle numeric features\n",
    "\n",
    "\n",
    "- Must convert categorical and ordinal features into numeric features (Encoding categorical features)\n",
    "    - Create dummy features\n",
    "    - Transform a categorical feature into a set of dummy features, each representing a unique category\n",
    "    - In the set of dummy features, 1 indicates that the observation belongs to that category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) use the get_dummies funtion to tranform the categorical variables into dummy variable with binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show the dummy varibals for the feature '2'(year)\n",
    "pd.get_dummies(train_data['2'],prefix='2').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of features to dummy\n",
    "# todummy_list = list(train_data.columns)\n",
    "# # Function to dummy all the categorical variables used for modeling\n",
    "# def dummy_df(df, todummy_list):\n",
    "#     for x in todummy_list:\n",
    "#         dummies = pd.get_dummies(df[x], prefix=x, dummy_na=False)\n",
    "#         df = df.drop(x, 1)\n",
    "#         df = pd.concat([df, dummies], axis=1)\n",
    "#     return df\n",
    "\n",
    "#better than just pd.get_dummies\n",
    "#X_train_dummies= pd.get_dummies(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train_dummies= dummy_df(train_data,todummy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print train_data.shape\n",
    "# print X_train_dummies.shape\n",
    "# X_train_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) LabelEncoder and OneHotEncoder\n",
    "It is used to transform non-numerical labels to numerical labels (or nominal categorical variables). \n",
    "Numerical labels are always between 0 and n_classes-1. \n",
    "OneHotEncoder takes as input categorical values encoded as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# X_train_label =pd.DataFrame()\n",
    "\n",
    "# for x in X_train.columns:\n",
    "    \n",
    "#     X_train_label[x] = le.fit_transform(train_data[x].astype('str'))\n",
    "\n",
    "# X_train_label.shape\n",
    "# X_train_label.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enc = OneHotEncoder()\n",
    "# enc.fit(df_label)  \n",
    "# enc.transform(df_label).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1) Handling missing data (for dense train data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Models can not handle missing data\n",
    "\n",
    "\n",
    "- Simplest solution\n",
    "    - Remove observations/features that have missing data\n",
    "    \n",
    "\n",
    "- But, removing missing data can introduce a lot of issues\n",
    "    - Data is randomly missing: potentially lose a lot of your data\n",
    "    - Data is non-randomly missing: in addition to losing data, you are also introducing potential biases\n",
    "    - Usually, this is a poor solution\n",
    "\n",
    "\n",
    "- An alternative solution is to use imputation\n",
    "    - Replace missing value with another value\n",
    "    - Strategies: mean, median, highest frequency value of given feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2) Handling missing data (for sparse train data)\n",
    "how to deal the missing data, simply using imputation here seems problematic!!!\n",
    "\n",
    "or then treat the train data as sparse and select the features\n",
    "seletion the important feature by feature selection algorithm??\n",
    "#### 1) Transformed into Sparse matrix\n",
    "#### 2) Feature selection\n",
    "seletion the important feature by feature selection algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "class sklearn.decomposition.SparseCoder(dictionary, transform_algorithm='omp', transform_n_nonzero_coefs=None, transform_alpha=None, split_sign=False, n_jobs=1)[source]\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "Feature selection is usually used as a pre-processing step before doing the actual learning. \n",
    "The recommended way to do this in scikit-learn is to use a sklearn.pipeline.Pipeline:\n",
    "\n",
    "In this snippet we make use of a sklearn.svm.LinearSVC coupled with sklearn.feature_selection.SelectFromModel to evaluate feature importances \n",
    "and select the most relevant features. \n",
    "Then, a sklearn.ensemble.RandomForestClassifier is trained on the transformed output, i.e. using only relevant features. \n",
    "You can perform similar operations with the other feature selection methods and also classifiers that provide a way to \n",
    "evaluate feature importances of course. \n",
    "See the sklearn.pipeline.Pipeline examples for more details.\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "clf.fit(X, y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impoove Features by adding the feature from the vin code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "VIN码详解：\n",
    "1）1～3位： （WMI：世界制造厂识别代码） 全球制造厂识别码，表示制造厂、品牌和类型；用来标识车辆制造厂的唯一性。通常占VIN代码的前三位；\n",
    "2) 4～9位（VDS：车辆说明部分）：\n",
    "说明车辆的一般特性，制造厂不用其中的一位或几位字符，就在该位置填入选定的字母或数字占位，其代号顺序由制造厂确定。\n",
    "第 4～8位(VDS)：车辆特征\n",
    "第四位：车身及底盘系列代码\n",
    "第五位：发动机类型代码\n",
    "第六～七位：车型代码\n",
    "第八位：乘员安全保护装置代码\n",
    "\n",
    "第9位：校验位  通过一定的算法防止输入错误；\n",
    "\n",
    "3)\n",
    "第10位：车型年份（ 车型年款代码 ），即厂家规定的型年（Model Year），不一定是实际生产的年份，但一般与实际生产的年份之差不超过1年；\n",
    "第11位：装配厂（总装工厂代码）；\n",
    "12～17位： 出厂顺序号代码 ，一般情况下，汽车召回都是针对某一顺序号范围内的车辆，即某一批次的车辆。\n",
    "\n",
    "'''\n",
    "# the features we want to extract fromt the vin code\n",
    "features_from_vin = ['vin_code_no4','vin_code_no5','vin_code_no6','vin_code_no7','vin_code_no8','vin_code_no10','vin_code_no11']\n",
    "\n",
    "feature_adding = pd.DataFrame(columns=features_from_vin)\n",
    "#extract the vin code into a series\n",
    "rack_no = raw_data['RACK_NO']\n",
    "\n",
    "# # for x in range(len(raw_data)):\n",
    "for x in range(10):\n",
    "    print rack_no[x]\n",
    "    temp=pd.DataFrame(columns=features_from_vin)\n",
    "   \n",
    "    for xx in range(len(features_from_vin)):\n",
    "        temp[temp.columns[xx]]=(rack_no[x][xx+3])\n",
    "    print temp\n",
    "    feature_adding.append(temp)\n",
    "#     pd.concat([feature_adding,temp],axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # combine the fatures from the feature list and vin code\n",
    "train_data_combined = pd.concat(feature_adding,train_data,axis = 1)\n",
    "train_data_CN_combined  = pd.concat(feature_adding,train_data_CN,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Easy way to deal with missing value in the train data and fealture selection\n",
    "### from the professional opinions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Question: what we can do to the categorical variable and missing value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) check the nan in the train data in each column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print train_data.isnull().sum()\n",
    "train_nan= pd.DataFrame(train_data.isnull().sum()) # tranform the series into a dataframe\n",
    "\n",
    "#set the threshold for the nan values ,we think the more import the feature is ,the less nan rate it will have \n",
    "nan_threshold = len(train_data)*0.1\n",
    "\n",
    "#show the columuns whose number of nan value is less than 2000, to choose the features in feture\n",
    "print len(train_nan[train_nan[train_nan.columns[0]]<nan_threshold])\n",
    "print train_nan[train_nan[train_nan.columns[0]]<nan_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) seletion the important and non nan features by nan value rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_nan[train_nan[train_nan.columns[0]]< nan_threshold]\n",
    "feature_list =list(train_nan[train_nan[train_nan.columns[0]]<nan_threshold].index)\n",
    "feature_list\n",
    "\n",
    "X_train_nanRate = train_data[feature_list]\n",
    "#sample train of 2000 observation \n",
    "#y_train = raw_data.ix[0:1999,'VEHICLE_CODE']\n",
    "\n",
    "#the whole train\n",
    "y_train = raw_data['VEHICLE_CODE']\n",
    "print X_train_nanRate.shape\n",
    "print len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "C:\\ProgramData\\Anaconda3\\envs\\python27\\lib\\site-packages\\sklearn\\cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
    "  % (min_labels, self.n_folds)), Warning)\n",
    "'''\n",
    "# some target in y_train just have one observation, which make the algorithm much hard to train\n",
    "# so we need to find out how many of these observations\n",
    "#the number of vehicle code from the original data of insurance company\n",
    "\n",
    "\n",
    "\n",
    "# print  raw_data['VEHICLE_CODE'].value_counts()\n",
    "print 'how many vehicle codes  from the insurance company ',len(vin_vehicleCode['VEHICLE_CODE'].value_counts())\n",
    "print 'how many vehicle codes  from the insurance company which also in our EPC databse:',len(raw_data['VEHICLE_CODE'].value_counts())\n",
    "print 'how many vehicle code which have no more than 3 cars from the insurance company:',np.sum(raw_data['VEHICLE_CODE'].value_counts()<4)\n",
    "\n",
    "\n",
    "\n",
    "#create the dictonary which have no more than 3 vehicles cresoponding to the vehicle code\n",
    "dict_countFew =[]\n",
    "temp_index = raw_data['VEHICLE_CODE'].value_counts().index\n",
    "temp_count = list(raw_data['VEHICLE_CODE'].value_counts())\n",
    "for index,count in zip(temp_index,temp_count):\n",
    "    \n",
    "    if count < 4:\n",
    "#         print index\n",
    "        dict_countFew.append(index)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "len(dict_countFew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the index of the obsevation which have few vehicle code cresponding\n",
    "index_countFew =[]\n",
    "for index,vehicleCode in enumerate(raw_data['VEHICLE_CODE']):\n",
    "    if vehicleCode in dict_countFew:\n",
    "        index_countFew.append(index)\n",
    "    else:\n",
    "        pass\n",
    "print 'the number of vehicle whose vehicle code have no more 3 in the train data',len(index_countFew)\n",
    "print 'we need to remove them from our train data to improve the model'\n",
    "# index_countFew       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(X_train_nanRate),len(y_train)\n",
    "print len(X_train_nanRate.drop(index_countFew))\n",
    "X_train_nanRate=X_train_nanRate.drop(index_countFew)\n",
    "y_train=y_train.drop(index_countFew)\n",
    "print len(X_train_nanRate),len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Handling missing data (for dense train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_nanRate.isnull()\n",
    "print 'the number of nan value in each column:'\n",
    "X_train_nanRate.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# have a look at the observation whose featuer '3' is null\n",
    "X_train_nanRate[X_train_nanRate['3'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the ways to handling the miss values:\n",
    "#### 1) we can delete the observations with nan value directly\n",
    "#### 2)we can inputer the the nan value with something else basen on the knowledge of the dataset\n",
    "#### 3) but more rational way is to treat the data as sparse data, we can improve the model by treating it as a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print X_train_nanRate.shape\n",
    "print X_train_nanRate.dropna(how='any').shape\n",
    "print X_train_nanRate.dropna(how='all').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### we fill the nan with 0000 as a new value for each feature ,then the estimator can handle them\n",
    "X_train_nanRate.fillna(value='0000',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4)Encoding the categorical vriables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the get_dummies funtion to tranform the categorical variables into dummy variable with binary values\n",
    "#Create a list of features to dummy\n",
    "todummy_list = list(X_train_nanRate.columns)\n",
    "# Function to dummy all the categorical variables used for modeling\n",
    "def dummy_df(df, todummy_list):\n",
    "    for x in todummy_list:\n",
    "        dummies = pd.get_dummies(df[x], prefix=x, dummy_na=False)\n",
    "        df = df.drop(x, 1)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "#better than just pd.get_dummies\n",
    "\n",
    "# X_train_dummies= pd.get_dummies(X_train)\n",
    "X_train_nanRate_dummies = dummy_df(X_train_nanRate,todummy_list)\n",
    "print X_train_nanRate.shape\n",
    "print X_train_nanRate_dummies.shape\n",
    "#there is nan value in this dense train data, we need to treat this problem when construct the estimator with imputer method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_nanRate_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#anothter way : use LabelEncoder to transform the categorical varialble to 0 to n-class-1 (or 1 to n-class)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X_train_nanRate_label =pd.DataFrame()\n",
    "\n",
    "for x in X_train_nanRate.columns:\n",
    "    # here we alread treat the nan value in the X_train as one kind of value\n",
    "    X_train_nanRate_label[x] = le.fit_transform(X_train_nanRate[x].astype('str'))\n",
    "\n",
    "X_train_nanRate_label.shape\n",
    "#X_train_label    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#then use OneHotEncode to encode the label into dummy variable \n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "enc = OneHotEncoder()\n",
    "enc.fit(X_train_nanRate_label)  \n",
    "X_train_nanRate_encoded=enc.transform(X_train_nanRate_label).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_nanRate_encoded[0,:]\n",
    "X_train_nanRate_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the dummy train data is same with the encoded train data\n",
    "X_train_nanRate_encoded[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NO need !!!\n",
    "# #use LabelEncoder to transform the response varialble to 0 to n-class-1 (or 1 to n-class)\n",
    "# y_le = LabelEncoder()\n",
    "# y_le.fit(y_train.astype('str'))\n",
    "# y_le.classes_\n",
    "# y_train_label = y_le.transform(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pre-Classification by the vin code information and platform imformation\n",
    "### 1. vin- code NO1-3: to choose which vehicle company it belong：\n",
    "lsj : 中国上海荣威\n",
    "### 2. plateform : to choose which plateform it  belong\n",
    "荣威550：\n",
    "### more information : to pre-classify the vin code into different prediction model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Model  selection \n",
    "### we want to try other models to find the best one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda:\n",
    "1.  parameter tuning for each model (Cross-validation or train/test split)\n",
    "\n",
    "  **Goal:** Select the best tuning parameters (aka \"hyperparameters\") for each on the iris dataset\n",
    "  - **sklearn.cross_validation.cross_val_score** \n",
    "      \n",
    "      loops for single parameter\n",
    "      \n",
    "  - **sklearn.grid_search.GridSearchCV ** More efficient parameter tuning \n",
    "    \n",
    "     Allows you to define a **grid of parameters** that will be **searched** using K-fold cross-validation\n",
    "  \n",
    "  - **sklearn.grid_search.RandomizedSearchCV** Reducing computational expense\n",
    "  \n",
    "     Searching many different parameters at once may be computationally infeasible\n",
    "    \n",
    "     `RandomizedSearchCV` searches a subset of the parameters, and you control the computational \"budget\"\n",
    "     \n",
    "2. model selection (Cross-validation or train/test split)\n",
    "\n",
    "  **Goal:** Compare the best models with each other\n",
    "  \n",
    "3. feature selection (Cross-validation)\n",
    "\n",
    "   **Tips**:Normally, feature engineering and selection occurs **before** cross-validation \n",
    "         if the data have lots of features,in computing practice we need to selection the features when preprocessing \n",
    "\n",
    "   **Goal**: Select whether somefeatures should be included in the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 The 1st mode :Random Forest \n",
    "### 1) Tuning the parameters with cross-validation\n",
    " -- A search consists of:\n",
    " - an estimator (regressor or classifier such as sklearn.svm.SVC());\n",
    " - a parameter space;\n",
    " - a method for searching or sampling candidates;\n",
    " - a cross-validation scheme; and\n",
    " - a score function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct a estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "\n",
    "# ##becaue when label the featrues into intergers, the nan value in each feature has benn already transfromed into one vule,so we don't need\n",
    "# ##the imputer function actually.\n",
    "# estimator = Pipeline([(\"imputer\", Imputer(missing_values=np.nan,\n",
    "#                                           strategy=\"most_frequent\",\n",
    "#                                           axis=0)),\n",
    "#                       (\"forest\", RandomForestClassifier(random_state=0,\n",
    "#                                                        n_estimators=num_tree,))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selction with train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_tree =100\n",
    "#instanciate the model\n",
    "estimator = RandomForestClassifier(n_estimators=num_tree,n_jobs=2)\n",
    "#split the train data into two parts\n",
    "train_X,test_X,train_y,test_y = train_test_split( X_train_nanRate_dummies,y_train,)\n",
    "\n",
    "#fit the estimator on the develpment part\n",
    "estimator.fit(train_X,train_y)\n",
    "\n",
    "#predict on the evalation part\n",
    "predicted =estimator.predict(test_X)\n",
    "\n",
    "# evaluate the estimator \n",
    "accuracy_score(predicted,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trainning the model on different size of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "randomForestClf_accuracy_scores=[]\n",
    "treeSize_range = [100,500,100]\n",
    "for num_tree in treeSize_range:\n",
    "    estimator = RandomForestClassifier(n_estimators=num_tree,n_jobs=2)\n",
    "    #split the train data into two parts\n",
    "    train_X,test_X,train_y,test_y = train_test_split( X_train_nanRate_dummies,y_train,)\n",
    "\n",
    "    #fit the estimator on the develpment part\n",
    "    estimator.fit(train_X,train_y)\n",
    "\n",
    "    #predict on the evalation part\n",
    "    predicted =estimator.predict(test_X)\n",
    "    # evaluate the estimator \n",
    "    acc = accuracy_score(predicted,test_y)\n",
    "    \n",
    "    randomForestClf_accuracy_scores.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the relationship between K and testing accuracy\n",
    "plt.plot(treeSize_range, randomForestClf_accuracy_scores)\n",
    "plt.xlabel('treeSize for randomForestClf')\n",
    "plt.ylabel('Testing Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Question : is there a problem just substitute the nan value with 'mean'??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acccessing the estimator with cross validation\n",
    "but this is too much time consumping for single computer!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\python27\\lib\\site-packages\\sklearn\\cross_validation.py:516: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.62501312904056061"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #using the X_train_dummies as train data of features\n",
    "# from sklearn.cross_validation  import cross_val_score\n",
    "# import sklearn.metrics\n",
    "# # # score = cross_val_score(estimator, X_train_dummies, y_train,scoring=sklearn.metrics.accuracy_score).mean()\n",
    "# score = cross_val_score(estimator, X_train_nanRate_dummies, y_train,n_jobs=2).mean()\n",
    "# score\n",
    "# # score = cross_val_score(estimator, X_train_nanRate_encoded, y_train,n_jobs=2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20093A176ASA76H3A00UA10SA40DB00LC00HC06CC08TC09CC10HC40VC91XD00GD01ME00TE55AF10LF11BF15KG10NH10DH12BH15AJ00FJ05EJ10PJ15AK19AK202K61BL10AL15AL18CL20AM05DM10DM19PM26AM27CM35AM40LM42CM43CN00AN06BN101N13MN22AN50LN62MN68AQ00BQ123Q19RS00PS15DS16DS35SS40PS42AT11AU00LU01UU039U04AU058U11XU35RZ10C'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vin='LSJW26H38AS072686'\n",
    "raw_data[raw_data['RACK_NO']==vin]['FEATURE_LIST'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning the parameters of RandomForestClassfier\n",
    "##### two variables： n_estimators && max_features\n",
    "the number of tree and the random partion of variables && the size of the random subsets of features to consider when splitting a node\n",
    "max_features=sqrt(n_features) default for classification tasks (where n_features is the number of features in the data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallizition :n_jobs=k \n",
    "     then computations are partitioned into k jobs, and run on k cores of the machine. \n",
    "     If n_jobs=-1 then all cores available on the machine are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation  import cross_val_score\n",
    "import sklearn.metrics\n",
    "num_trees = range(100,1001,100)\n",
    "n_features = train_data.shape[1]\n",
    "RandomForestClf_scores =[]\n",
    "\n",
    "for n in num_trees:\n",
    "    num_tree = n\n",
    "    estimator = RandomForestClassifier(n_estimators=num_tree,n_job=1)\n",
    "    score = cross_val_score(estimator, X_train_dummies,scoring=sklearn.metrics.accuracy_score, y_train,n_jobs=1).mean()\n",
    "    RandomForestClf_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot the performance for each num of tree\n",
    "RandomForestClf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importances\n",
    "In practice those estimates are stored as an attribute named feature_importances_ on the fitted model. \n",
    "This is an array with shape (n_features,) whose values are positive and sum to 1.0. \n",
    "The higher the value, the more important is the contribution of the matching feature to the prediction function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2) Model Acccessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## 2.2 AdaBoost\n",
    "### 1) Tuning the parameter with cross-validation\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.cross_validation  import cross_val_score\n",
    "\n",
    "# num_trees = range(100,2001,100)\n",
    "# n_features = train_data.shape[1]\n",
    "# AdaBoostClf_scores =[]\n",
    "\n",
    "# for n in num_trees:\n",
    "    \n",
    "#     num_tree = n\n",
    "#     estimator = AdaBoostClassifier()\n",
    "#     score = cross_val_score(estimator, X_train_dummies, y_train,n_jobs=-1).mean()\n",
    "#     AdaBoostClf_scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Gradient Tree Boosting¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Classification with more than 2 classes requires the induction of n_classes regression trees at each iteration, thus, the total number of induced trees equals n_classes * n_estimators. For datasets with a large number of classes we strongly recommend to use RandomForestClassifier as an alternative to GradientBoostingClassifier ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Fit classifier with interation 500 times\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y,test_y = train_test_split(X_train_nanRate_dummies,y_train,test_size=0.2)\n",
    "GB_clf = GradientBoostingClassifier(n_estimators=500,learning_rate=0.1)\n",
    "GB_clf.fit(train_X,train_y)\n",
    "GB_clf.score(test_X,test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Tunning the parameters\n",
    "# #set the parameters we want to tunning\n",
    "# params = {'n_estimators': [100,500,1000], 'max_depth': [1:4], 'subsample': 0.5,\n",
    "#           'learning_rate': 0.01, 'min_samples_leaf': 1}\n",
    "\n",
    "# ####instance the estimator\n",
    "# GBclf = GradientBoostingClassifier(**params)\n",
    "\n",
    "# ###using the randomizedSearchCV\n",
    "# RandomizedSearchCV(GB_clf,param_distributions=params,n_jobs=2)\n",
    "\n",
    "# ### check the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.3 Model: XG booosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part3: Model  evaluation\n",
    "Use a model evaluation procedure to estimate how well a model will generalize to out-of-sample data\n",
    "\n",
    "Requires a model evaluation metric to quantify the model performance\n",
    "## Agenda:\n",
    "after we have selection the model,the parameter and features in part 1 and part2\n",
    "\n",
    "1. Model evaluation (Train/test split,K-fold cross-validation)\n",
    "\n",
    "2. Model evaluation metrics\n",
    " - Regression problems: Mean Absolute Error, Mean Squared Error, Root Mean Squared Error\n",
    " - Classification problems: \n",
    " \n",
    "       A) Classification accuracy\n",
    "   \n",
    "       B) confusion matrix\n",
    "   \n",
    "       C) ROC curve adn Area Under the Curve (AUC) differ from classification accuracy\n",
    "       \n",
    "       **Question:** Wouldn't it be nice if we could see how sensitivity and specificity are affected by various thresholds, without actually changing the threshold?\n",
    "       **Answer:** Plot the ROC curve!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>A00</th>\n",
       "      <th>A01</th>\n",
       "      <th>A02</th>\n",
       "      <th>A03</th>\n",
       "      <th>A05</th>\n",
       "      <th>...</th>\n",
       "      <th>U35</th>\n",
       "      <th>U40</th>\n",
       "      <th>U41</th>\n",
       "      <th>U42</th>\n",
       "      <th>Z00</th>\n",
       "      <th>Z10</th>\n",
       "      <th>Z15</th>\n",
       "      <th>Z16</th>\n",
       "      <th>Z90</th>\n",
       "      <th>Z99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>3A17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6ASA</td>\n",
       "      <td>76H3</td>\n",
       "      <td>A00U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U35R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z10C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 349 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      2     3   5     6     7   A00  A01  A02  A03  A05 ...   U35  U40  U41  \\\n",
       "0  2009  3A17 NaN  6ASA  76H3  A00U  NaN  NaN  NaN  NaN ...  U35R  NaN  NaN   \n",
       "\n",
       "   U42  Z00   Z10 Z15  Z16 Z90 Z99  \n",
       "0  NaN  NaN  Z10C NaN  NaN NaN NaN  \n",
       "\n",
       "[1 rows x 349 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bulid a funciton to interpret a vin code into a feature list by our EPC system\n",
    "def vinInterpretedToFeatureList(vin):\n",
    "    #samle is a feature list\n",
    "    temp_data = pd.DataFrame(np.nan, index=[0],columns=dict_column)\n",
    "    temp_data_CN = pd.DataFrame(np.nan, index=[0],columns=dict_column)\n",
    "    \n",
    "    feature_list = raw_data[raw_data['RACK_NO']==vin]['FEATURE_LIST'].values[0]\n",
    "    #feature_list = raw_data['FEATURE_LIST'][279]\n",
    "    # print feature_list\n",
    "    # print temp_data\n",
    "    # print type(dict_column)\n",
    "    # print type(feature_list[0])\n",
    "    count =0\n",
    "    for xx in range(0,len(feature_list),4):\n",
    "\n",
    "        if (feature_list[xx] in ['2','3','5','6','7']) and (count <=6):\n",
    "            count = count+1\n",
    "            feature =feature_list[xx]\n",
    "            feature_code =feature_list[xx:xx+4]\n",
    "            #print xx,count,feature_code\n",
    "            #print feature_code\n",
    "\n",
    "            if tm_feature[tm_feature['FEATURE_CODE']==feature_code].values.shape[0]==0:\n",
    "                #some feature_code like 76E3 in 279th observation ,but not exsit in the tm_feature['FEATURE_CODE], we assume it is just the correce \n",
    "                #feature_code,but fail to find the chinese feature name\n",
    "                if tm_feature[tm_feature['FEATURE_CODE']==feature_code[0:3]].values.shape[0]==0:\n",
    "                    temp_data[feature] = np.nan\n",
    "                    temp_data_CN[feature]= np.nan\n",
    "\n",
    "                #some feature_code lke G10N in 0th observation, but not exsit in the tm_feature['FEATURE_CODE],\n",
    "                #we assume it is the same as G10    \n",
    "                else:\n",
    "                    temp_data[feature] = feature_code[0:3]\n",
    "                    temp_data_CN[feature]= feature_code[0:3]\n",
    "\n",
    "            #print feature_code\n",
    "            else:\n",
    "                temp_data[feature] = feature_code\n",
    "                #print feature_code, tm_feature[tm_feature['FEATURE_CODE']==feature_code]\n",
    "                temp_data_CN[feature]= tm_feature[tm_feature['FEATURE_CODE']==feature_code].values[(0,4)]\n",
    "                #.values which change a data frame into np array\n",
    "\n",
    "        elif any(feature_list[xx:xx+3] == x for x in dict_column):\n",
    "            #print any(feature_list[xx:xx+3] == x for x in dict_column)\n",
    "            feature = feature_list[xx:xx+3]\n",
    "            feature_code = feature_list[xx:xx+4]\n",
    "\n",
    "            if tm_feature[tm_feature['FEATURE_CODE']==feature_code].values.shape[0]==0:\n",
    "                #some feature_code like 76E3 in 279th observation ,but not exsit in the tm_feature['FEATURE_CODE], we assume it is just the correce \n",
    "                #feature_code,but fail to find the chinese feature name\n",
    "                if tm_feature[tm_feature['FEATURE_CODE']==feature_code[0:3]].values.shape[0]==0:\n",
    "                    temp_data[feature] = np.nan\n",
    "                    temp_data_CN[feature]= np.nan\n",
    "\n",
    "\n",
    "                #some feature_code lke G10N in 0th observation, but not exsit in the tm_feature['FEATURE_CODE],\n",
    "                #we assume it is the same as G10    \n",
    "                else:\n",
    "                    temp_data[feature] = feature_code[0:3]\n",
    "                    temp_data_CN[feature]= feature_code[0:3]\n",
    "\n",
    "            #print feature_code\n",
    "            else:\n",
    "                temp_data[feature] = feature_code\n",
    "                #print feature_code, tm_feature[tm_feature['FEATURE_CODE']==feature_code]\n",
    "                temp_data_CN[feature]= tm_feature[tm_feature['FEATURE_CODE']==feature_code].values[(0,4)]\n",
    "                #.values which change a data frame into np array\n",
    "\n",
    "    return temp_data\n",
    "\n",
    "vin='LSJW26H38AS072686'\n",
    "vinInterpretedToFeatureList(vin)\n",
    "# we can use other 'FEATURE_CN_CHINA' OR 'NUM' HERE   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>A00</th>\n",
       "      <th>A10</th>\n",
       "      <th>A40</th>\n",
       "      <th>B00</th>\n",
       "      <th>C00</th>\n",
       "      <th>C06</th>\n",
       "      <th>...</th>\n",
       "      <th>Q19</th>\n",
       "      <th>S00</th>\n",
       "      <th>S15</th>\n",
       "      <th>S35</th>\n",
       "      <th>S40</th>\n",
       "      <th>T11</th>\n",
       "      <th>U01</th>\n",
       "      <th>U05</th>\n",
       "      <th>U11</th>\n",
       "      <th>U35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2  3  6   7  A00  A10  A40  B00  C00  C06 ...   Q19  S00  S15  S35  S40  \\\n",
       "0  2  0  0  27   11    3    0    0    7    0 ...     0    5    3    0    1   \n",
       "\n",
       "   T11  U01  U05  U11  U35  \n",
       "0    0    1    3    1    0  \n",
       "\n",
       "[1 rows x 39 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vinInterpretedToFeatureLables(vin):\n",
    "    temp_data =vinInterpretedToFeatureList(vin)\n",
    "    temp_label=pd.DataFrame()\n",
    "    for x in X_train.columns:\n",
    "        le = LabelEncoder()\n",
    "        #we need to use the train data here to transform the data, but acturally we just need the dictory of the feature to transform\n",
    "        le.fit(X_train[x].astype('str'))\n",
    "        temp_label[x] = le.transform(temp_data[x])\n",
    "    return temp_label\n",
    "        \n",
    "\n",
    "vinInterpretedToFeatureLables(vin)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainning the estimator with the full train data\n",
    "clf = RandomForestClassifier(random_state=11,n_estimators=100)\n",
    "estimator = clf.fit(X_train_label, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RWAABD0022'], dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict with the estimator\n",
    "test_vin_feature =vinInterpretedToFeatureLables(vin)\n",
    "estimator.predict(test_vin_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vinEstimator(vin):\n",
    "    \n",
    "    test_vin_feature =vinInterpretedToFeatureLables(vin)\n",
    "    vehicle_code = estimator.predict(test_vin_feature)\n",
    "    return vehicle_code\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# while True: \n",
    "#     vin_code= raw_input('please input the 17 bits vin code: ') # why I can't use the function input('')\n",
    "#     #vin_code ='LSJW26H38AS072686'\n",
    "#     if vin_code == 'quit' or vin_code == 'q':\n",
    "#         break\n",
    "# #         print 'hi'\n",
    "\n",
    "#     elif len(vin_code) != 17:\n",
    "#         print 'the vin code is not correct'\n",
    "#     else:\n",
    "#         vehicle_code_predicted = vinEstimator(vin_code)\n",
    "#         vehicle_code = raw_data[raw_data['RACK_NO']==vin_code]['VEHICLE_CODE'].values[0] #get the value from a series\n",
    "#         print 'the predicted vehicle code is '+ vehicle_code_predicted\n",
    "#         print 'the actual vehicle code is '+vehicle_code\n",
    "#         if vehicle_code_predicted == vehicle_code:\n",
    "#             print 'Wow,congratulation!! your prediction is correct!'\n",
    "#         else:\n",
    "#             print 'It is a pity, your prediction is wrong!!!'+'\\n'\n",
    "#             print 'if you want to quit,please input: quit or q'+'\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RWAABD0022'], dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vinEstimator('LSJW26H33AS079268')\n",
    "vinEstimator('LSJW26H36AS082617')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw_data[0:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improving the predict model\n",
    "### one way:     adding into more features from the 17 bits vin code\n",
    "### another way: using the information from the 17 bits vin code to pre-classify the model\n",
    "### one way:     selection better classifitor: such as SVM, Nural network,(I dont thing knn will improve the performance,logistics                   classification is for linear boundary) \n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part3: Model persistence\n",
    "After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain. \n",
    "The following section gives you an example of how to persist a model with pickle. We’ll also review a few security and maintainability issues when working with pickle serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
